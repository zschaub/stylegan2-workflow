{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will take a dataset of images, run them through TSNE to group them up (if enabled) then create a stylegan2 model with or without ADA.\n",
    "\n",
    "Below are setting to choose when running this workflow. Make sure before running to have all images you want to use in a folder inside of the images folder. For example have a folder inside images called mona-lisa filled with pictures of different versions of the Mona Lisa. Please have the subfolder have no whitespaces in the name.\n",
    "\n",
    "If TSNE is enable the program will halt after processing the images and ask you to choose which cluster to use. The clusters will be in the folder clusters.\n",
    "\n",
    "Before running make sure your kernal is set to Python 3 (TensorFlow 1.15 Python 3.7 GPU Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'mona-lisa'\n",
    "\n",
    "use_ada = True\n",
    "use_tsne = False\n",
    "use_spacewalk = True\n",
    "\n",
    "\n",
    "# Crop Settings\n",
    "# Choose center or no-crop\n",
    "# TODO: Add random\n",
    "crop_type = 'no-crop'\n",
    "resolution = 512\n",
    "\n",
    "\n",
    "# TSNE Settings\n",
    "# Choose number of clusters to make or None for auto clustering\n",
    "num_clusters = None\n",
    "\n",
    "\n",
    "# ADA Settings\n",
    "knum = 10\n",
    "\n",
    "\n",
    "# Spacewalk Settings\n",
    "fps = 24\n",
    "seconds = 10\n",
    "#Leave seeds = None for random seeds or \n",
    "# enter a list in the form of [int, int, int..] to define the seeds\n",
    "seeds = None\n",
    "# set walk_type to 'line', 'sphere', 'noiseloop', or 'circularloop'\n",
    "walk_type = 'sphere'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python>=3.4.1.15 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (4.3.0.36)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.18.5)\n",
      "Requirement already satisfied: matplotlib>=3.2.0 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: tqdm==4.45.0 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (4.45.0)\n",
      "Requirement already satisfied: Pillow==7.0.0 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (7.0.0)\n",
      "Requirement already satisfied: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (0.22.2.post1)\n",
      "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (1.4.1)\n",
      "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (1.4.0)\n",
      "Requirement already satisfied: torchvision==0.5.0 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (0.5.0)\n",
      "Requirement already satisfied: gap-stat>=2 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (2.0.1)\n",
      "Requirement already satisfied: opensimplex in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 12)) (0.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn==0.22.2.post1->-r requirements.txt (line 6)) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from torchvision==0.5.0->-r requirements.txt (line 9)) (1.15.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (from gap-stat>=2->-r requirements.txt (line 10)) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.2.0->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.2.0->-r requirements.txt (line 3)) (2.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.2.0->-r requirements.txt (line 3)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.2.0->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas->gap-stat>=2->-r requirements.txt (line 10)) (2021.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import train\n",
    "from PIL import Image, ImageFile, ImageOps\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(pil_img, res):\n",
    "    return pil_img.resize((res, res))\n",
    "\n",
    "def crop_center(pil_img, res):\n",
    "    \n",
    "    crop = res\n",
    "    \n",
    "    img_width, img_height = pil_img.size\n",
    "    if img_width < crop:\n",
    "        crop = img_width\n",
    "    if img_height < crop:\n",
    "        crop = img_height\n",
    "        \n",
    "    a = (img_width - crop) // 2\n",
    "    b = (img_height - crop) // 2\n",
    "    c = (img_width + crop) // 2\n",
    "    d = (img_height + crop) // 2\n",
    "        \n",
    "    cropped_image = pil_img.crop((a,b,c,d))\n",
    "    return resize(cropped_image, res)\n",
    "\n",
    "def no_crop(pil_img, res):\n",
    "    color = [0, 0, 0]\n",
    "    \n",
    "    img_width, img_height = pil_img.size\n",
    "    if img_width < img_height:\n",
    "        top = 0\n",
    "        bottom = 0\n",
    "        left =  math.ceil((img_height - img_width) / 2.0)\n",
    "        right =  math.floor((img_height - img_width) / 2.0)\n",
    "    else:\n",
    "        top =  math.ceil((img_height - img_width) / 2.0)\n",
    "        bottom =  math.floor((img_height - img_width) / 2.0)\n",
    "        left = 0\n",
    "        right = 0\n",
    "    \n",
    "    border_image = ImageOps.expand(pil_img, border=(left, top, right, bottom), fill='white')\n",
    "    return resize(border_image, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_dir = './images/'\n",
    "tmp_dir = './tmp/'\n",
    "\n",
    "image_dir = os.path.join(image_dir, dataset_name)\n",
    "tmp_dir = os.path.join(tmp_dir, dataset_name)\n",
    "\n",
    "\n",
    "if not os.path.exists(tmp_dir):\n",
    "    os.makedirs(tmp_dir)\n",
    "else:\n",
    "    try:\n",
    "        shutil.rmtree(tmp_dir)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s : %s\" % (dir_path, e.strerror))\n",
    "    os.makedirs(tmp_dir)\n",
    "    \n",
    "\n",
    "for filename in os.listdir(image_dir):\n",
    "    file_extension = os.path.splitext(filename)[-1]\n",
    "    if file_extension != '.jpg' and file_extension != '.png':\n",
    "        print(file_extension)\n",
    "        continue\n",
    "    \n",
    "    image_path = os.path.join(image_dir, filename)\n",
    "    image = Image.open(image_path)\n",
    "    mode = image.mode\n",
    "    if str(mode) != 'RGB':\n",
    "        continue\n",
    "    if crop_type == \"center\":\n",
    "        image = crop_center(image, resolution)\n",
    "    if crop_type == \"no-crop\":\n",
    "        image = no_crop(image, resolution)\n",
    "        \n",
    "    tmp_path = os.path.join(tmp_dir, filename)\n",
    "    image.save(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSNE is not in use\n"
     ]
    }
   ],
   "source": [
    "if use_tsne:\n",
    "    !python tsne.py --path={tmp_dir}\n",
    "else:\n",
    "    print('TSNE is not in use')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If TSNE is enabled when it is finished running check the Clusters folder and choose the cluster you want to use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tsne:\n",
    "    clusters = []\n",
    "    while True:\n",
    "        x = input(\"Enter a cluster you want to use or Enter to continue: \")\n",
    "        if x == '':\n",
    "            break\n",
    "        clusters.append(int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tsne:\n",
    "    save_dir = os.path.join(\"./tmp\", str(dataset_name + \"_clusters\"))\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    else:\n",
    "        try:\n",
    "            shutil.rmtree(save_dir)\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s : %s\" % (save_dir, e.strerror))\n",
    "        os.makedirs(save_dir)\n",
    "                            \n",
    "    for c in clusters:\n",
    "        cluster_dir = os.path.join(\"./clusters\", dataset_name, str(\"cluster_\" + str(c)))\n",
    "        \n",
    "        for filename in os.listdir(cluster_dir):\n",
    "            file = os.path.join(cluster_dir, filename)\n",
    "            new_file = os.path.join(save_dir, filename)\n",
    "            shutil.move(file, new_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "Loading images from \"./tmp/mona-lisa\"\n",
      "Creating dataset \"./datasets/mona-lisa\"\n",
      "Added 529 images.                       \n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "\n",
      "Training options:\n",
      "{\n",
      "  \"G_args\": {\n",
      "    \"func_name\": \"training.networks.G_main\",\n",
      "    \"fmap_base\": 16384,\n",
      "    \"fmap_max\": 512,\n",
      "    \"mapping_layers\": 2,\n",
      "    \"num_fp16_res\": 4,\n",
      "    \"conv_clamp\": 256\n",
      "  },\n",
      "  \"D_args\": {\n",
      "    \"func_name\": \"training.networks.D_main\",\n",
      "    \"mbstd_group_size\": 4,\n",
      "    \"fmap_base\": 16384,\n",
      "    \"fmap_max\": 512,\n",
      "    \"num_fp16_res\": 4,\n",
      "    \"conv_clamp\": 256\n",
      "  },\n",
      "  \"G_opt_args\": {\n",
      "    \"beta1\": 0.0,\n",
      "    \"beta2\": 0.99,\n",
      "    \"learning_rate\": 0.0025\n",
      "  },\n",
      "  \"D_opt_args\": {\n",
      "    \"beta1\": 0.0,\n",
      "    \"beta2\": 0.99,\n",
      "    \"learning_rate\": 0.0025\n",
      "  },\n",
      "  \"loss_args\": {\n",
      "    \"func_name\": \"training.loss.stylegan2\",\n",
      "    \"r1_gamma\": 1.6384\n",
      "  },\n",
      "  \"augment_args\": {\n",
      "    \"class_name\": \"training.augment.AdaptiveAugment\",\n",
      "    \"tune_heuristic\": \"rt\",\n",
      "    \"tune_target\": 0.6,\n",
      "    \"apply_func\": \"training.augment.augment_pipeline\",\n",
      "    \"apply_args\": {\n",
      "      \"xflip\": 1,\n",
      "      \"rotate90\": 1,\n",
      "      \"xint\": 1,\n",
      "      \"scale\": 1,\n",
      "      \"rotate\": 1,\n",
      "      \"aniso\": 1,\n",
      "      \"xfrac\": 1,\n",
      "      \"brightness\": 1,\n",
      "      \"contrast\": 1,\n",
      "      \"lumaflip\": 1,\n",
      "      \"hue\": 1,\n",
      "      \"saturation\": 1\n",
      "    }\n",
      "  },\n",
      "  \"num_gpus\": 4,\n",
      "  \"image_snapshot_ticks\": 50,\n",
      "  \"network_snapshot_ticks\": 50,\n",
      "  \"train_dataset_args\": {\n",
      "    \"path\": \"./datasets/mona-lisa\",\n",
      "    \"max_label_size\": 0,\n",
      "    \"use_raw\": false,\n",
      "    \"resolution\": 512,\n",
      "    \"mirror_augment\": false,\n",
      "    \"mirror_augment_v\": false\n",
      "  },\n",
      "  \"metric_arg_list\": [\n",
      "    {\n",
      "      \"name\": \"fid50k_full\",\n",
      "      \"class_name\": \"metrics.frechet_inception_distance.FID\",\n",
      "      \"max_reals\": null,\n",
      "      \"num_fakes\": 50000,\n",
      "      \"minibatch_per_gpu\": 8,\n",
      "      \"force_dataset_args\": {\n",
      "        \"shuffle\": false,\n",
      "        \"max_images\": null,\n",
      "        \"repeat\": false,\n",
      "        \"mirror_augment\": false\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"metric_dataset_args\": {\n",
      "    \"path\": \"./datasets/mona-lisa\",\n",
      "    \"max_label_size\": 0,\n",
      "    \"use_raw\": false,\n",
      "    \"resolution\": 512,\n",
      "    \"mirror_augment\": false,\n",
      "    \"mirror_augment_v\": false\n",
      "  },\n",
      "  \"total_kimg\": 10,\n",
      "  \"minibatch_size\": 32,\n",
      "  \"minibatch_gpu\": 8,\n",
      "  \"G_smoothing_kimg\": 10.0,\n",
      "  \"G_smoothing_rampup\": 0.05,\n",
      "  \"run_dir\": \"./training-runs/00000-mona-lisa-res512-auto4-kimg10\"\n",
      "}\n",
      "\n",
      "Output directory:  ./training-runs/00000-mona-lisa-res512-auto4-kimg10\n",
      "Training data:     ./datasets/mona-lisa\n",
      "Training length:   10 kimg\n",
      "Resolution:        512\n",
      "Number of GPUs:    4\n",
      "\n",
      "Creating output directory...\n",
      "Loading training set...\n",
      "Image shape: [3, 512, 512]\n",
      "Label shape: [0]\n",
      "\n",
      "Constructing networks...\n",
      "Setting up TensorFlow plugin \"fused_bias_act.cu\": Loading... Done.\n",
      "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Loading... Done.\n",
      "\n",
      "G                             Params    OutputShape         WeightShape     \n",
      "---                           ---       ---                 ---             \n",
      "latents_in                    -         (?, 512)            -               \n",
      "labels_in                     -         (?, 0)              -               \n",
      "epochs                        1         ()                  ()              \n",
      "epochs_1                      1         ()                  ()              \n",
      "G_mapping/Normalize           -         (?, 512)            -               \n",
      "G_mapping/Dense0              262656    (?, 512)            (512, 512)      \n",
      "G_mapping/Dense1              262656    (?, 512)            (512, 512)      \n",
      "G_mapping/Broadcast           -         (?, 16, 512)        -               \n",
      "dlatent_avg                   -         (512,)              -               \n",
      "Truncation/Lerp               -         (?, 16, 512)        -               \n",
      "G_synthesis/4x4/Const         8192      (?, 512, 4, 4)      (1, 512, 4, 4)  \n",
      "G_synthesis/4x4/Conv          2622465   (?, 512, 4, 4)      (3, 3, 512, 512)\n",
      "G_synthesis/4x4/ToRGB         264195    (?, 3, 4, 4)        (1, 1, 512, 3)  \n",
      "G_synthesis/8x8/Conv0_up      2622465   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
      "G_synthesis/8x8/Conv1         2622465   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
      "G_synthesis/8x8/Upsample      -         (?, 3, 8, 8)        -               \n",
      "G_synthesis/8x8/ToRGB         264195    (?, 3, 8, 8)        (1, 1, 512, 3)  \n",
      "G_synthesis/16x16/Conv0_up    2622465   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
      "G_synthesis/16x16/Conv1       2622465   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
      "G_synthesis/16x16/Upsample    -         (?, 3, 16, 16)      -               \n",
      "G_synthesis/16x16/ToRGB       264195    (?, 3, 16, 16)      (1, 1, 512, 3)  \n",
      "G_synthesis/32x32/Conv0_up    2622465   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
      "G_synthesis/32x32/Conv1       2622465   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
      "G_synthesis/32x32/Upsample    -         (?, 3, 32, 32)      -               \n",
      "G_synthesis/32x32/ToRGB       264195    (?, 3, 32, 32)      (1, 1, 512, 3)  \n",
      "G_synthesis/64x64/Conv0_up    2622465   (?, 512, 64, 64)    (3, 3, 512, 512)\n",
      "G_synthesis/64x64/Conv1       2622465   (?, 512, 64, 64)    (3, 3, 512, 512)\n",
      "G_synthesis/64x64/Upsample    -         (?, 3, 64, 64)      -               \n",
      "G_synthesis/64x64/ToRGB       264195    (?, 3, 64, 64)      (1, 1, 512, 3)  \n",
      "G_synthesis/128x128/Conv0_up  1442561   (?, 256, 128, 128)  (3, 3, 512, 256)\n",
      "G_synthesis/128x128/Conv1     721409    (?, 256, 128, 128)  (3, 3, 256, 256)\n",
      "G_synthesis/128x128/Upsample  -         (?, 3, 128, 128)    -               \n",
      "G_synthesis/128x128/ToRGB     132099    (?, 3, 128, 128)    (1, 1, 256, 3)  \n",
      "G_synthesis/256x256/Conv0_up  426369    (?, 128, 256, 256)  (3, 3, 256, 128)\n",
      "G_synthesis/256x256/Conv1     213249    (?, 128, 256, 256)  (3, 3, 128, 128)\n",
      "G_synthesis/256x256/Upsample  -         (?, 3, 256, 256)    -               \n",
      "G_synthesis/256x256/ToRGB     66051     (?, 3, 256, 256)    (1, 1, 128, 3)  \n",
      "G_synthesis/512x512/Conv0_up  139457    (?, 64, 512, 512)   (3, 3, 128, 64) \n",
      "G_synthesis/512x512/Conv1     69761     (?, 64, 512, 512)   (3, 3, 64, 64)  \n",
      "G_synthesis/512x512/Upsample  -         (?, 3, 512, 512)    -               \n",
      "G_synthesis/512x512/ToRGB     33027     (?, 3, 512, 512)    (1, 1, 64, 3)   \n",
      "---                           ---       ---                 ---             \n",
      "Total                         28700649                                      \n",
      "\n",
      "\n",
      "D                    Params    OutputShape         WeightShape     \n",
      "---                  ---       ---                 ---             \n",
      "images_in            -         (?, 3, 512, 512)    -               \n",
      "labels_in            -         (?, 0)              -               \n",
      "512x512/FromRGB      256       (?, 64, 512, 512)   (1, 1, 3, 64)   \n",
      "512x512/Conv0        36928     (?, 64, 512, 512)   (3, 3, 64, 64)  \n",
      "512x512/Conv1_down   73856     (?, 128, 256, 256)  (3, 3, 64, 128) \n",
      "512x512/Skip         8192      (?, 128, 256, 256)  (1, 1, 64, 128) \n",
      "256x256/Conv0        147584    (?, 128, 256, 256)  (3, 3, 128, 128)\n",
      "256x256/Conv1_down   295168    (?, 256, 128, 128)  (3, 3, 128, 256)\n",
      "256x256/Skip         32768     (?, 256, 128, 128)  (1, 1, 128, 256)\n",
      "128x128/Conv0        590080    (?, 256, 128, 128)  (3, 3, 256, 256)\n",
      "128x128/Conv1_down   1180160   (?, 512, 64, 64)    (3, 3, 256, 512)\n",
      "128x128/Skip         131072    (?, 512, 64, 64)    (1, 1, 256, 512)\n",
      "64x64/Conv0          2359808   (?, 512, 64, 64)    (3, 3, 512, 512)\n",
      "64x64/Conv1_down     2359808   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
      "64x64/Skip           262144    (?, 512, 32, 32)    (1, 1, 512, 512)\n",
      "32x32/Conv0          2359808   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
      "32x32/Conv1_down     2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
      "32x32/Skip           262144    (?, 512, 16, 16)    (1, 1, 512, 512)\n",
      "16x16/Conv0          2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
      "16x16/Conv1_down     2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
      "16x16/Skip           262144    (?, 512, 8, 8)      (1, 1, 512, 512)\n",
      "8x8/Conv0            2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
      "8x8/Conv1_down       2359808   (?, 512, 4, 4)      (3, 3, 512, 512)\n",
      "8x8/Skip             262144    (?, 512, 4, 4)      (1, 1, 512, 512)\n",
      "4x4/MinibatchStddev  -         (?, 513, 4, 4)      -               \n",
      "4x4/Conv             2364416   (?, 512, 4, 4)      (3, 3, 513, 512)\n",
      "4x4/Dense0           4194816   (?, 512)            (8192, 512)     \n",
      "Output               513       (?, 1)              (512, 1)        \n",
      "---                  ---       ---                 ---             \n",
      "Total                28982849                                      \n",
      "\n",
      "Exporting sample images...\n",
      "Replicating networks across 4 GPUs...\n",
      "Initializing augmentations...\n",
      "Setting up optimizers...\n",
      "Constructing training graph...\n",
      "Finalizing training ops...\n",
      "------------------------------------------------------------------------\n",
      "WARNING: Using slow fallback implementation for inter-GPU communication.\n",
      "Please use TensorFlow 1.14 on Linux for optimal training performance.\n",
      "------------------------------------------------------------------------\n",
      "Initializing metrics...\n",
      "Training for 10 kimg...\n",
      "\n",
      "tensorflow-1-15-gpu--ml-p3-8xlarge-17ea97aaf69e14e0f1f5454d7cf3:496:720 [0] NCCL INFO NET/Socket : Using [0]veth-app0-2:169.255.255.2<0>\n",
      "tensorflow-1-15-gpu--ml-p3-8xlarge-17ea97aaf69e14e0f1f5454d7cf3:496:720 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\n",
      "\n",
      "tensorflow-1-15-gpu--ml-p3-8xlarge-17ea97aaf69e14e0f1f5454d7cf3:496:720 [0] external/nccl_archive/src/misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\n",
      "tick 0     kimg 0.1      time 29m 18s      sec/tick 63.6    sec/kimg 496.96  maintenance 1694.0 gpumem 10.0  augment 0.000\n",
      "Evaluating metrics...\n",
      "Calculating real image statistics for fid50k_full...\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = os.path.join(\"./datasets\", dataset_name)\n",
    "\n",
    "if use_ada and use_tsne:\n",
    "    image_dir = os.path.join(\"./tmp\", str(dataset_name + \"_clusters\"))\n",
    "    !python dataset_tool.py create_from_images {dataset_dir} {image_dir}\n",
    "    !python train.py --outdir=./training-runs --gpus=4 --res={resolution} --data={dataset_dir} --kimg={knum}\n",
    "elif use_ada:\n",
    "    image_dir = os.path.join(\"./tmp\", dataset_name)\n",
    "    !python dataset_tool.py create_from_images {dataset_dir} {image_dir}\n",
    "    !python train.py --outdir=./training-runs --gpus=4 --res={resolution} --data={dataset_dir} --kimg={knum}\n",
    "else:\n",
    "    print(\"ADA is not in use\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 1.15 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/tensorflow-1.15-gpu-py37-cu110-ubuntu18.04-v8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
